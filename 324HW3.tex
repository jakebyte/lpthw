\documentclass{report}
\usepackage{bbold}
\usepackage{amsmath}
\usepackage[margin=.75in]{geometry}

\begin{document}
ENEE 324 HW \#3 \\
Jacob Besteman-Street \\
\today \\
\begin{enumerate}
\item Suppose that $X$ is a discrete random variable (rv) with range $ S_X \subset N := \{1,2,3, ... \}$. Prove
$$ E[X] = \sum_{i \in N} P[X \geq i]$$
Starting with the definition of $E[X]$:
$$ E[X] = \sum_{x \in S_x} x \cdot p_X(x) $$
To get here, we rewrite $\sum_{i \in N} P[X \geq i]$
$$ \sum_{i \in N}P[X \geq i] = \sum_{i \in N} \sum_{x \geq i}p_X(x)   $$
From the definition of $N$,
$$\sum_{i \in N} \sum_{x \geq i}p_X(x) = \sum_{i = 1}^{\infty}\sum_{x \geq i}p_X(x)$$
Logically, this means that for each value of $x = i$, $p_X(x)$ will be added $i$ times. For example, $p_X(1)$ will only be included in the first summation. $p_X(2)$ will be added in the first and second. And thus this simplifies to the definition of the expectation. \\
Mathematically, it needs a few more steps:
$$\sum_{i = 1}^{\infty}\sum_{x \geq i}p_X(x) = \sum_{x = 1}^{\infty}\sum_{i =1}^{x}p_X(x) = \sum_{x = 1}^{\infty}p_X(x)\sum_{i =1}^{x}1 = \sum_{x = 1}^{\infty}p_X(x) \cdot x $$
Based on the definition $ S_X \subset N := \{1,2,3, ... \}$,
$$ \sum_{x = 1}^{\infty}p_X(x) \cdot x = \sum_{x \in S_x} x \cdot p_X(x) = E[X]$$
\item Suppose that $X$ and $Y$ are two discrete rvs, and $ S_X = S_Y = \{1,-1\}$
\begin{itemize}
\item[(a)] Suppose that $E[X] = E[Y] = 0$. Show that $p_{X,Y}(1,1) = p_{X,Y}(-1,-1)$ and $p_{X,Y}(1,-1) = p_{X,Y}(-1,1)$.
Since each rv has only two possible values, 1 and -1, each must have equal probabiltiy 0.5 in order for the expectation to be 0. If $X$ and $Y$ were independent, then this would be simple: $p_{X,Y}(1,1) = p_{X,Y}(-1,-1) = p_{X,Y}(1,-1) = p_{X,Y}(-1,1) = 0.25$ \\
However, even if they aren't independent, their conditional probabilities must be symmetric in order to give an expectation of 0. In other words:
$$p_Y(1) = p_{Y|X}(1|1)+p_{Y|X}(1|-1) = 0.5 \text{ and } p_Y(-1) = p_{Y|X}(-1|1)+p_{Y|X}(-1|-1) = 0.5$$
Since the expecation is 0, if $p_{Y|X}(1|1) > p_{Y|X}(1|-1)$ (Y is more likely to be 1 if X is 1 than if it is -1), then $p_{Y|X}(-1|1)<p_{Y|X}(-1|-1)$ is necessary to ensure that the total $p_Y(1) = p_Y(-1) = 0.5$. More specifically, if $p_{Y|X}(1|1) \neq p_{Y|X}(1|-1)$, then $$p_{Y|X}(1|1) - p_{Y|X}(1|-1) = p_{Y|X}(-1|-1)-p_{Y|X}(-1|1)$$
Rearranging the earlier equations to solve for $p_{Y|X}(1|1),p_{Y|X}(1|-1),p_{Y|X}(-1|1),\text{ and }p_{Y|X}(-1|-1)$, then subsituting into the equality above and simplifying yeilds:
$$p_{Y|X}(1|1)  = p_{Y|X}(-1|-1) \text{ and } p_{Y|X}(1|-1) =p_{Y|X}(-1|1)$$
Each side of the above equations can be multiplied by $p_X(1) = 0.5$ or $p_X(-1) = 0.5$ to turn the conditional probability into the desired joint probability, proving that $p_{X,Y}(1,1) = p_{X,Y}(-1,-1)$ and $p_{X,Y}(1,-1) = p_{X,Y}(-1,1)$.
\item[(b)] Assume $E[X] = E[Y] = 0$. Let $p=2p_{X,Y}(1,1)$. Find $Var(X)$ and $Var(Y)$. Write $Cov(X,Y)$ in terms of $p$.\\
$$Var(X) =  E[X^2]-E[X]^2 = 1-0 = 1 \text{ and }Var(Y) =  E[Y^2]-E[Y]^2 = 1-0 = 1$$
$$Cov(X,Y) = \sigma_{X,Y}=E[(X-E[X])(Y-E[Y])] = E[X \cdot Y]$$
$$ E[X \cdot Y]= \sum x\cdot y \cdot p_{X,Y}(x,y) = p_{X,Y}(1,1)-p_{X,Y}(1,-1)-p_{X,Y}(-1,1)+p_{X,Y}(-1,-1)$$
Since we showed that $p_{X,Y}(1,1) = p_{X,Y}(-1,-1)$ and $p_{X,Y}(1,-1) = p_{X,Y}(-1,1)$ above, this can be simplified.
$$p_{X,Y}(1,1)-p_{X,Y}(1,-1)-p_{X,Y}(-1,1)+p_{X,Y}(-1,-1) = 2 p_{X,Y}(1,1) - 2 p_{X,Y}(1,-1)$$
In addition,
$$ p_{X,Y}(1,1)+p_{X,Y}(1,-1)+p_{X,Y}(-1,1)+p_{X,Y}(-1,-1) = 1 = 2 p_{X,Y}(1,1) + 2 p_{X,Y}(1,-1)$$
$$2 p_{X,Y}(1,-1) = 1-2 p_{X,Y}(1,1)$$
Substituting this into $\sigma_{X,Y}$ and replacing $2 p_{X,Y}(1,1)$ with $p$ gives:
$$Cov(X,Y) = \sigma_{X,Y}= 2 p_{X,Y}(1,1)-(1-2 p_{X,Y}(1,1)) p-(1-p)= 2p-1$$
\end{itemize}


\item Let $X$ and $Y$ be two discrete rv with joint PMF
$$ p_{X,Y}(x,y) = \left. \begin{cases}
0.1 & x = 1, 2, \cdots , 10, y = 1, 2, \cdots , 10, \\
0 & otherwise
\end{cases} \right.$$
\begin{itemize}
\item[(a)] What is the PMF of $W = min(X,Y)$? \\
There are 100 possible combinations for $x = 1, 2, \cdots , 10, y = 1, 2, \cdots , 10$. Only one has a minimum value of 10. Nineteen have a minimum value of 1. And so on in between.
$$ p_W(w) = \left. \begin{cases}
0.19 & w = 1 \\
0.17 & w = 2 \\
0.15 & w = 3 \\
0.13 & w = 4 \\
0.11 & w = 5 \\
0.9 & w = 6 \\
0.7 & w = 7 \\
0.5 & w = 8 \\
0.3 & w = 9 \\
0.1 & w = 10 \\
0 & otherwise
\end{cases} \right.$$
\item[(b)] What is the PMF of $Z = max(X,Y)$?\\
Same result as W, but with the order reversed.
$$ p_Z(z) = \left. \begin{cases}
0.19 & z = 10 \\
0.17 & z = 9 \\
0.15 & z = 8 \\
0.13 & z = 7 \\
0.11 & z = 6 \\
0.9 & z = 5 \\
0.7 & z = 4 \\
0.5 & z = 3 \\
0.3 & z = 2 \\
0.1 & z = 1 \\
0 & otherwise
\end{cases} \right.$$
\end{itemize}
\item Tom and Mary want to have 2 girls together. Each time they have a baby, it is a girl with a probability
of 0.6. They stop having any more babies when they have two girls. Let $N_1$ be the number of boys
till the first girl and $N_T$ the total number of children they have together.
\begin{itemize}
  \item[(a)] Let $B=$ \{third baby is a boy\}. What is the conditional joint PMF $p_{N_1, N_T|B}(n_1,n_T)$?\\
This limits the available values for $N_1$ and $N_T$. Because the third child is a boy, $N_1 \neq 2$ and $N_T > 3$. This sets the minimum for $N_1$ and reduces the possible permutations for where the first girl was born.\\
$$p_{N_1, N_T|B}(n_1,n_T) = \left. \begin{cases}
(0.316)(0.4)^{n_T - 4}(0.6) & n_1 = 0,1,  n_T \geq 4  \text{ (One of the 1st two children is a girl)} \\
(0.4)^{n_T - 3}(0.6)^2 & n_1 \geq 3,  n_T \geq n_1 + 2  \text{ (Neither of the 1st two children is a girl)} \\
0 & otherwise
\end{cases} \right.$$
\item[(b)] Compute $E[N_1|B]$ and $E[N_T|B]$.\\
$N_1$ is related to a geometric rv, the birth of the first girl. Ignoring $B$, $E[N_1 +1] = \frac{1}{p} = \frac{5}{3}$. However, the expectation is calculated as a sum, $E[X] = \sum_{x \in S_x} x \cdot p_X(x)$.
The effect of $B$ is to remove one of the possible values of $N_1$. Specifically, $N_1 \neq 2$ or the first girl can't be the third child. Since the original expectation is a sum of all possible values, the conditional expectation can be obtained by subtracting the disallowed one.
$$E[N_1 +1] =  \frac{\frac{5}{3} - 3p_{N_1+1}(3)}{1-p_{N_1+1}(3)} = \frac{\frac{5}{3} - 3(0.4^2 \cdot 0.6)}{1-(0.4^2 \cdot 0.6)} = \frac{\frac{5}{3} - 0.288}{1-0.096} = 1.525$$
This is the expected value for the birth of the first girl given $B$. $E[N_1|B]$ will therefore be $1.525 - 1 = 0.525$.\\
$N_T$ is a pascal rv, with an expected value of $\frac{2}{0.6} = 3\frac{1}{3}$. The condition $B$ removes the possibility of $N_T = 2,3$, so these values should be removed from the expectation and the probability of the others bumped up.
$$ E[N_T|B] = \frac{3\frac{1}{3} - (2p_{N_T}(2) + 3p_{N_T}(3))}{1 -(p_{N_T}(2) + p_{N_T}(3)) } = \frac{3\frac{1}{3} - (2(0.6)^2 + 3(0.4)(0.6)^2)}{1 -((0.6)^2 + (0.4)(0.6)^2) } =  \frac{3\frac{1}{3} - (0.72 + 0.432)}{1 -(0.36 + 0.144) } = \frac{2.181}{0.496} = 4.40 $$
\item[(c)] Find the conditional PMFs $p_{N_1|N_T}(n_1|5)$ and $p_{N_T|N_!}(n_T|2)$.\\
If $N_T = 5$, then $N_1$ can be 0, 1, 2 or 3. Each of these is equally likely.
$$p_{N_1|N_T}(n_1|5) = \left. \begin{cases}
0.25 & n_1 = 0,1,2,3 \\
0 & otherwise
\end{cases} \right.$$
From a given value of $N_1$, $N_T$ is a geometric rv with parameter 0.6.
$$p_{N_T|N_!}(n_T|2) = \left. \begin{cases}
(0.4)^{n_T - 4}(0.6) & n_T = 4, 5, 6,... \\
0 & otherwise
\end{cases} \right.$$
\end{itemize}
\item The number of dry days following a rainy day is geometrically distributed with a parameter that
depends on the amount of rain we have. Suppose that it is raining today. The amount of rain $Y$ we
will get today is 0.2, 0.5, and 0.8 inches with probability 0.3, 0.3, and 0.4, respectively, e.g., $P$(we
get 0.2 inches of rain today) = 0.3. Suppose that the parameter of the geometric rv $X$, which is the
number of dry days we will have after today before another rainy day, is equal to the amount of rain
we have today.
\begin{itemize}
\item[(a)] Find the joint PMF of $X$ and $Y$. \\
Working from $p_{X,Y}(x,y) = p_{X|Y}(x|y)p_Y(y)$,
$$ p_{X|Y}(x|y) = \left. \begin{cases}
(1-y)^{x-1}y & x = 0,1,... \\
0 & otherwise
\end{cases} \right.$$

Substituting in the values for $y$ and multipling by their respective probabilities:
$$ p_{X,Y}(x,y) = \left. \begin{cases}
(0.3)(0.8)^{x-1}(0.2) & y = 0.2, x = 0,1,2,... \\
(0.3)(0.5)^{x} & y = 0.5, x = 0,1,2,... \\
(0.4)(0.2)^{x-1}(0.8) & y = 0.8, x = 0,1,2,... \\
0 & otherwise
\end{cases} \right.$$
\item[(b)] Compute E[X]. \\
The expectation of a geometric series is $\frac{1}{p}$. Since the parameter for $X$ depends on $Y$:
$$E[X] = \sum_{Y \in S_Y} p_Y(y)\frac{1}{y} = (0.3)\frac{1}{0.2} + (0.3)\frac{1}{0.5} + (0.4)\frac{1}{0.8} = 1.5 + 0.6 + 0.48 = 2.58$$
\item[(c)] Compute the correlation coefficient of $X$ and $Y$. \\
Correlation Coefficient $\rho_{X,Y}= \frac{Cov(X,Y)}{Var(X) \cdot Var(Y)}$. For this, we need the Expectation and Variance of both $X$ and $Y$, as well as the Covariance. $E[X]$ was calculated in part (b).
$$E[Y] = (0.3)(0.2) + (0.3)(0.5) + (0.4)(0.8) = 0.06 + 0.15 + 0.32 = 0.53$$
$$E[Y^2] = (0.3)(0.2)^2 + (0.3)(0.5)^2 + (0.4)(0.8)^2 = 0.012 + 0.075 + 0.256 = 0.343 \text{ and } (E[Y])^2 = 0.281 $$
$$\text{Therefore } Var[Y] = 0.343 -  0.281 = 0.062$$
Finding $Var[X]$ is worse. $(E[X])^2 = 6.66$. That's the easy part. To find $E[X^2]$, break it into a sum:
$$E[X^2] = \sum_{Y\in S_Y} E[X^2|Y=y]p_Y(y) $$
This breaks it into the respective geometric terms. Then use $E[(X^2)]=E(X)+[E(X(X−1)]$. Then find the solution on stack exchange.
$$\text{For a geometric rv X, } E[X(X−1)] = \frac{2(1-p)}{p^2}\text{ and }E[(X^2)]=E(X)+[E(X(X−1)] = \frac{1}{p} + \frac{2(1-p)}{p^2} =\frac{2-p}{p^2} $$
In this case, $p=y$, so we sum the results for each value of $y$ multiplied by their respective probability:
$$E[X^2] = \sum_{Y\in S_Y} E[X^2|Y=y]p_Y(y) = (0.3)\frac{2-0.2}{0.2^2} +(0.3)\frac{2-0.5}{0.5^2} +(0.4)\frac{2-0.8}{0.8^2} = 13.5+1.8+.75 = 16.05$$
$$\text{Finally, }Var[X] = E[X^2] - E[X]^2 = 16.05 - 6.66 = 9.39$$
Now we just need the covariance, $\sigma_{X,Y}=E[(X-E[X])(Y-E[Y])]$. And I don't even know where to begin with this. But the Correlation Coefficient will be the Covariance devided by the product of the variances.
\end{itemize}
\item Suppose that next year’s revenue $R$ of a company depends on its rating $X$ this year. If the rating $X$ is
“3”, its revenue has a binomial distribution with parameter $(n, p) = (100, 0.5)$. If the rating is “2”, the
revenue is geometrically distributed with parameter 1/30. If its rating is “1”, the revenue is uniformly
distributed on \{0, 1,..., 50\}.\\
Because the probabitities for the values of $X$ are not specified, I will be using $p_X(x)$ for $x \in [1,2,3]$ in my equations.
\begin{itemize}
\item[(a)] Find the joint PMF of $R$ and $X$.\\
Like 5(a) but with R changing type.
$$ p_{X,R}(x,r) = \left. \begin{cases}
(p_X(3))\left(\!
    \begin{array}{c}
      100 \\
      r
    \end{array}
  \!\right)(0.5)^{100} & x= 3, r = \{0,1,2,...,100\} \\
(p_X(2))(1-\frac{1}{30})^{r-1}(\frac{1}{30}) & x = 2, r = 0,1,2,... \\
(p_X(1))(\frac{1}{51}) & x = 1, r = \{0,1,2,...,50\} \\
0 & otherwise
\end{cases} \right.$$
\item[(b)] Compute E[R].\\
Like 5(b), treat this as a sum of $E[R|X]$ multiplied by the probabilities of $X$:
$$E[R] = \sum_{X \in S_X} E[R|X]p_X(x) = E[R|X=1]p_X(1)+ E[R|X=2]p_X(2)+ E[R|X=3]p_X(3)$$
$$E[R] = 25p_X(1) + 30p_X(2) + 50p_X(3)  $$
\item[(c)] Find the conditional PMF $p_{X|R}(x|100)$. \\
For this, we need the Bayes Rule: $p_{X|R}(x|r) = p_{R|X}(r|x)\frac{p_X(x)}{p_R(r)}$. Thus,$p_{X|R}(1|100) = p_{R|X}(100|1)\frac{p_X(1)}{p_R(100)}$, $p_{X|R}(2|100) = p_{R|X}(100|2)\frac{p_X(2)}{p_R(100)}$ and $p_{X|R}(3|100) = p_{R|X}(100|3)\frac{p_X(3)}{p_R(100)}$.
The first of these is clearly 0 because $R$ cannot be 100 if $X$ is 1. Start by calculating the conditional probabilities for $R$:
$$p_{R|X}(100|3) = 0.5^{100} = 7.89 \cdot 10^{-31} \text{ This is essentially 0}$$
$$p_{R|X}(100|2) = (\frac{29}{30})^{99}\frac{1}{30} = 0.00116$$
 $p_{R|X}(100|2)$ is small, but greater than 0 and more than $10^{27}$ times larger than $p_{R|X}(100|3)$. Therefore, we can ignore the rest of the Bayes Rule calculation and conclude that if $R=100$, then $X=2$.
 $$p_{X|R}(x|100) = \left. \begin{cases}
 1 & x=2 \\
 0 & Otherwise
 \end{cases} \right.$$


\end{itemize}
\end{enumerate}

 \end{document}
